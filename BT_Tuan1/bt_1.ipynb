{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N19DCCN204: Phạm Văn Thuận\n",
    "- N19DCNC126: Lê Hoài Nhân\n",
    "- N19DCCN136: Nguyễn Trọng Tín"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file and re-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_processing_words(vocab):\n",
    "  process_words = []\n",
    "  for line in vocab:\n",
    "    splitted_line = line.split()[:-1] # removing sign '/' at the end of the line\n",
    "\n",
    "    for word in splitted_line:\n",
    "      if word.strip() and not word.isdigit():\n",
    "        process_words.append(word.lower())\n",
    "  process_words.sort()\n",
    "  return process_words\n",
    "\n",
    "def remove_stop_words(docs):\n",
    "  english_stop_words = stopwords.words('english')\n",
    "  new_docs = []\n",
    "  for doc in docs:\n",
    "    new_docs.append(' '.join([word for word in doc.split() if word not in english_stop_words]))\n",
    "  return new_docs\n",
    "\n",
    "\n",
    "def re_processing_docs(docs):\n",
    "  processed_docs = []\n",
    "  # removing digit and break line ('\\n)\n",
    "  st1_pr_docs = list(filter(lambda y: not y.isdigit(),map(lambda x: x[:-1], docs)))\n",
    "  # splitting allow forward flash\n",
    "  table_remove_punctuation = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "  docs_tmp = []\n",
    "  for doc in st1_pr_docs:\n",
    "    if doc.endswith('/'):\n",
    "      processed_docs.append(' '.join(docs_tmp))\n",
    "      docs_tmp.clear()\n",
    "    else:\n",
    "      docs_tmp.append(' '.join([w.lower().translate(table_remove_punctuation) for w in doc.split()]))\n",
    "  return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = None \n",
    "with open('./dataset/term-vocab', 'r') as f:\n",
    "  vocab = f.readlines()\n",
    "\n",
    "  f.close()\n",
    "processed_vocab = re_processing_words(vocab)\n",
    "print(processed_vocab[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compact memories flexible capacities digital data storage system capacity bits random sequential access described\n"
     ]
    }
   ],
   "source": [
    "docs = None\n",
    "with open('./dataset/doc-text', 'r') as f:\n",
    "  docs = f.readlines()\n",
    "\n",
    "  f.close()\n",
    "processed_docs = remove_stop_words(re_processing_docs(docs))\n",
    "print(processed_docs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Marked matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "marked_matrix = np.zeros(shape=(len(processed_vocab), len(processed_docs)), dtype=np.uint8)\n",
    "# marked\n",
    "for w, word in enumerate(processed_vocab):\n",
    "  for d, doc in enumerate(processed_docs):\n",
    "    if word in doc:\n",
    "      marked_matrix[w, d] = 1\n",
    "\n",
    "print(marked_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup index table\n",
    "table_vocab_idx = {}\n",
    "for i, word in enumerate(processed_vocab):\n",
    "  table_vocab_idx[word] = i\n",
    "\n",
    "print(table_vocab_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Reverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "reverted_index = reduce(lambda acc, word: {**acc, word: []} , list(set([word for word in (' '.join(processed_docs)).split() if word.strip()])), {})\n",
    "reverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(processed_docs):\n",
    "  for word in list(set([word for word in doc.split() if doc.strip()])):\n",
    "    reverted_index[word].append(i + 1)\n",
    "\n",
    "for key in reverted_index:\n",
    "    reverted_index[key] = list(set(reverted_index[key]))\n",
    "    reverted_index[key].sort()\n",
    "\n",
    "list_tmp = list(reverted_index.items())\n",
    "list_tmp.sort(key=lambda x: x[0])\n",
    "\n",
    "reverted_index = dict(list_tmp)\n",
    "with open('./result/reverted_index.txt', 'w') as f:\n",
    "  for key, value in reverted_index.items():\n",
    "    f.write(str(key) + '\\n' + ','.join(map(lambda x: str(x), value)) + '\\n\\n/')\n",
    "\n",
    "  f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-processing query docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement dielectric constant liquids use microwave techniques\n"
     ]
    }
   ],
   "source": [
    "# re-processing query doc\n",
    "query_docs = None\n",
    "with open('./dataset/query-text', 'r') as f:\n",
    "  query_docs = f.readlines()\n",
    "\n",
    "  f.close()\n",
    "\n",
    "processed_query_docs = remove_stop_words(re_processing_docs(query_docs))\n",
    "print(processed_query_docs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query with marked matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "with open('./result/marked_matrix.txt', 'w') as f:\n",
    "  for i, query_doc in enumerate(processed_query_docs):\n",
    "    f.write(str(i + 1) + '\\n')\n",
    "    tmp.clear()\n",
    "    for query_word in query_doc.split():\n",
    "      if query_word in processed_vocab:\n",
    "        tmp.append(marked_matrix[table_vocab_idx.get(query_word)])\n",
    "\n",
    "    if tmp:\n",
    "      answers = reduce(lambda acc, x: np.bitwise_and(np.array(acc), np.array(x), dtype=np.uint8), tmp)\n",
    "      for i, isMarked in enumerate(answers):\n",
    "        if isMarked == 1:\n",
    "          f.write(str(i + 1) + ',')\n",
    "    f.write('\\n/\\n')\n",
    "  \n",
    "  f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query with reverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(posting_list1, posting_list2):\n",
    "  doc_id_list = []\n",
    "  i_pl1, i_pl2 = 0, 0\n",
    "  len_pl1 = len(posting_list1)\n",
    "  len_pl2 = len(posting_list2)\n",
    "  while(i_pl1 < len_pl1 and i_pl2 < len_pl2):\n",
    "    if posting_list1[i_pl1] == posting_list2[i_pl2]:\n",
    "      doc_id_list.append(posting_list1[i_pl1])\n",
    "      i_pl1 += 1\n",
    "      i_pl2 += 1\n",
    "    elif posting_list1[i_pl1] < posting_list2[i_pl2]:\n",
    "      i_pl1 += 1\n",
    "    else:\n",
    "      i_pl2 += 1\n",
    "  return doc_id_list\n",
    "\n",
    "def print_answer(query):\n",
    "  def print_result(doc_id_answers):\n",
    "    print('The docIDs satisfied the query: ', query)\n",
    "    if not doc_id_answers:\n",
    "      print('Don\\'t have any answers')\n",
    "    for doc_id in doc_id_answers:\n",
    "      print(doc_id)\n",
    "  return print_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(reverted_index.keys())\n",
    "with open('./result/query_reverted_index.txt', 'w') as f:\n",
    "  for i, query_doc in enumerate(processed_query_docs):\n",
    "    f.write(str(i + 1) + ' ')\n",
    "    query_words = [word for word in query_doc.split() if word.strip() and word in keys]\n",
    "\n",
    "    intersect = reverted_index[query_words[0]]\n",
    "    query_words = query_words[1:]\n",
    "\n",
    "    print_result = print_answer(query_doc)\n",
    "\n",
    "    for word in query_words:\n",
    "      intersect = intersection(intersect, reverted_index[word])\n",
    "    \n",
    "    f.write(str(','.join(map(lambda x: str(x), intersect)) + '\\n'))\n",
    "    f.write('/\\n')\n",
    "    print_result(intersect)\n",
    "\n",
    "  f.close()  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(reverted_index.keys())\n",
    "with open('./result/query_reverted_index_opt.txt', 'w') as f:\n",
    "  for i, query_doc in enumerate(processed_query_docs):\n",
    "    f.write(str(i + 1) + ' ')\n",
    "    query_words = [[word, len(reverted_index[word])] for word in query_doc.split() if word.strip() and word in keys]\n",
    "\n",
    "    # sorting by len of posting list\n",
    "    query_words.sort(key=lambda x: x[1])\n",
    "    query_words = list(map(lambda x: x[0], query_words))\n",
    "\n",
    "\n",
    "    intersect = reverted_index[query_words[0]]\n",
    "    query_words = query_words[1:]\n",
    "\n",
    "    print_result = print_answer(query_doc)\n",
    "\n",
    "    for word in query_words:\n",
    "      intersect = intersection(intersect, reverted_index[word])\n",
    "    \n",
    "    f.write(str(','.join(map(lambda x: str(x), intersect)) + '\\n'))\n",
    "    f.write('/\\n')\n",
    "    print_result(intersect)\n",
    "\n",
    "  f.close()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_skip(skip_step, len_list):\n",
    "  def check(p_list):\n",
    "    return (skip_step + p_list) < len_list\n",
    "  return check\n",
    "\n",
    "def intersection_with_skip_pointer(list1, list2, skip_step = 1):\n",
    "  answer = []\n",
    "  p_list1, p_list2 = 0, 0\n",
    "  len_p_list1 = len(list1)\n",
    "  len_p_list2 = len(list2)\n",
    "\n",
    "  has_skip_list1 = has_skip(skip_step, len_p_list1)\n",
    "  has_skip_list2 = has_skip(skip_step, len_p_list2)\n",
    "\n",
    "  while p_list1 < len_p_list1 and p_list2 < len_p_list2:\n",
    "    if list1[p_list1] == list2[p_list2]:\n",
    "      answer.append(list1[p_list1])\n",
    "      p_list1 += 1\n",
    "      p_list2 += 1\n",
    "    elif list1[p_list1] < list2[p_list2]:\n",
    "      if has_skip_list1(p_list1) and list1[p_list1 + skip_step] <= list2[p_list2]:\n",
    "        while has_skip_list1(p_list1) and list1[p_list1 + skip_step] <= list2[p_list2]:\n",
    "          p_list1 += skip_step\n",
    "      else: p_list1 += 1\n",
    "    else:\n",
    "      if has_skip_list2(p_list2) and list2[p_list2 + skip_step] <= list1[p_list1]:\n",
    "        while has_skip_list2(p_list2) and list2[p_list2 + skip_step] <= list1[p_list1]:\n",
    "          p_list2 += skip_step\n",
    "      else: p_list2 += 1\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "intersection_with_skip_pointer([2,5,8,41,48,64,128], [1,2,3,8,11,17,21,31], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(reverted_index.keys())\n",
    "with open('./result/query_reverted_index_skip_pointer.txt', 'w') as f:\n",
    "  for i, query_doc in enumerate(processed_query_docs):\n",
    "    f.write(str(i + 1) + ' ')\n",
    "    query_words = [word for word in query_doc.split() if word.strip() and word in keys]\n",
    "\n",
    "    intersect = reverted_index[query_words[0]]\n",
    "    query_words = query_words[1:]\n",
    "\n",
    "    print_result = print_answer(query_doc)\n",
    "\n",
    "    for word in query_words:\n",
    "      intersect = intersection_with_skip_pointer(intersect, reverted_index[word], int(math.sqrt(len(reverted_index[word]))))\n",
    "    \n",
    "    f.write(str(','.join(map(lambda x: str(x), intersect))) + '\\n')\n",
    "    f.write('/\\n')\n",
    "    print_result(intersect)\n",
    "\n",
    "  f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
